import os

from langfuse.openai import openai

import src.models as models
from src.prompts import (
    ORDER_SYSTEM_MESSAGE,
)
from src.tools import (
    DirectReturnException,
    llm_tools_map,
    order_dataset_tools,
    product_dataset_tools,
)
from src.utils import create_logger, handle_function_calls, log_execution_time

TEMPERATURE = 0
MAX_COMPLETION_TOKENS = 2048

logger = create_logger(logger_name="llm", log_file="api.log", log_level="info")

model = os.environ.get("CHAT_MODEL")
if not model:
    logger.error("CHAT_MODEL environment variable is not set.")
    raise ValueError("CHAT_MODEL environment variable is not set.")

client = openai.OpenAI()


@log_execution_time(logger=logger)
def handle_user_chat(thread: models.Thread) -> models.Message:
    """
    Process a user's chat thread and generate a response using language model tools.

    Args:
        thread (models.Thread): The conversation thread containing user messages.

    Returns:
        models.Message: A message object containing the LLM's response.

    Raises:
        DirectReturnException: If the LLM flow needs to be returned directly to the user.
        Exception: For any errors during generating responses.
    """
    input_messages = thread.messages
    try:
        while True:
            # Generate initial completion
            response = create_completion(
                thread=thread, tools=order_dataset_tools + product_dataset_tools
            )
            thread.messages.append(response)

            if not response.tool_calls:
                break
            thread.messages = handle_function_calls(
                function_map=llm_tools_map,
                response_message=response,
                messages=thread.messages,
            )

        return models.Message(role=response.role, content=response.content)

    except DirectReturnException as e:
        logger.info(f"LLM returned flow to the user: {e.message}")
        raise
    except Exception as e:
        logger.error(
            f"Error generating responses for chat:\n'{input_messages}'\n\nError:\n{e}"
        )
        raise


@log_execution_time(logger=logger)
def create_completion(
    thread: models.Thread,
    system_message: str = ORDER_SYSTEM_MESSAGE,
    tools: list = order_dataset_tools,
):
    """
    Create a completion response from the language model based on the conversation thread.

    Args:
        thread (models.Thread): The conversation thread containing all messages.
        system_message (str): The system message to be included in the prompt.
        tools (list): A list of tools available for LLM to use.

    Returns:
        models.Message: The response message generated by the LLM.

    Raises:
        Exception: If the LLM fails to generate a response.
    """
    try:
        logger.info(
            f" create_completition |{thread.messages = }\n\n{system_message = }"
        )
        completion = client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_message},
                *thread.messages,
            ],
            model=model,
            temperature=TEMPERATURE,
            max_tokens=MAX_COMPLETION_TOKENS,
            tools=tools,
            session_id=thread.id,
        )

        logger.info(f" create_completion | {completion = }")
        response = completion.choices[0].message
        return response
    except Exception as e:
        logger.error(
            f" create_completion | Error generating responses for chat '{thread.messages}': {e}"
        )
        raise
